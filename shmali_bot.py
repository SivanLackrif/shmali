import csv
import random
import requests
import pandas as pd
import base64
import re
import json
from datetime import datetime, timedelta
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, ContextTypes, MessageHandler, filters
import openai
import asyncio
from difflib import SequenceMatcher

# Configuration
OPENAI_API_KEY = 'OPENAI_API_KEY'
TELEGRAM_TOKEN = "TELEGRAM_TOKEN"
CLIENT_ID = 'CLIENT_ID'
CLIENT_SECRET = 'CLIENT_SECRET'
DATASET_CSV = 'podcast_dataset.csv'

# Initialize OpenAI
openai.api_key = OPENAI_API_KEY

class SpotifyAPI:
    def __init__(self, client_id, client_secret):
        self.client_id = client_id
        self.client_secret = client_secret
        self.token = None
        self.token_expiry = None

    def get_access_token(self):
        if self.token and self.token_expiry and datetime.now() < self.token_expiry:
            return self.token
        
        try:
            auth_str = f"{self.client_id}:{self.client_secret}"
            b64_auth = base64.b64encode(auth_str.encode()).decode()
            response = requests.post(
                'https://accounts.spotify.com/api/token',
                headers={'Authorization': f'Basic {b64_auth}'},
                data={'grant_type': 'client_credentials'},
                timeout=10
            )
            data = response.json()
            self.token = data['access_token']
            self.token_expiry = datetime.now() + timedelta(seconds=data.get('expires_in', 3600))
            return self.token
        except Exception as e:
            print(f"Error getting Spotify token: {e}")
            return None

    def search_podcasts(self, query, max_duration=None, limit=10, language_preference=None):
        """×—×™×¤×•×© ×¤×•×“×§××¡×˜×™× ×¢× ×”×¢×“×¤×ª ×©×¤×”"""
        if not self.get_access_token():
            return []
            
        headers = {'Authorization': f'Bearer {self.token}'}
        
        # ×‘× ×™×™×ª ×©××™×œ×ª×ª ×—×™×¤×•×© ×¢×œ ×‘×¡×™×¡ ×”×¢×“×¤×ª ×”×©×¤×”
        if language_preference == 'hebrew':
            search_queries = [
                f'{query} ×¤×•×“×§××¡×˜',
                f'×¤×•×“×§××¡×˜ {query}',
                f'{query} podcast hebrew',
                f'{query} ×¢×‘×¨×™×ª'
            ]
        elif language_preference == 'english':
            search_queries = [
                f'{query} english podcast -hebrew -×¢×‘×¨×™×ª',
                f'english {query} podcast USA',
                f'{query} podcast UK',
                f'{query} podcast american',
                f'{query} english language'
            ]
        else:
            search_queries = [f'{query} podcast', f'{query} ×¤×•×“×§××¡×˜']
        
        all_results = []
        
        for search_query in search_queries:
            try:
                market = 'US' if language_preference == 'english' else 'IL'
                
                response = requests.get(
                    'https://api.spotify.com/v1/search',
                    headers=headers,
                    params={
                        'q': search_query,
                        'type': 'show',
                        'limit': limit * 2,
                        'market': market
                    },
                    timeout=10
                )
                
                if response.status_code != 200:
                    continue
                    
                shows = response.json().get('shows', {}).get('items', [])
                
                for show in shows:
                    # ×‘×“×™×§×ª ×©×¤×”
                    languages = show.get('languages', [])
                    name = show.get('name', '').lower()
                    description = show.get('description', '').lower()
                    
                    # ×¡×™× ×•×Ÿ ×œ×¤×™ ×”×¢×“×¤×ª ×©×¤×”
                    if language_preference == 'hebrew':
                        if not ('he' in languages or 'iw' in languages or 
                               any(hebrew_word in name for hebrew_word in ['×¢×‘×¨×™×ª', '×™×©×¨××œ', '×¤×•×“×§××¡×˜']) or
                               any(hebrew_word in description for hebrew_word in ['×¢×‘×¨×™×ª', '×™×©×¨××œ'])):
                            if 'en' in languages and not any(hebrew_char in name + description for hebrew_char in '××‘×’×“×”×•×–×—×˜×™×›×œ×× ×¡×¢×¤×¦×§×¨×©×ª'):
                                continue
                    
                    elif language_preference == 'english':
                        has_hebrew = any(hebrew_char in name + description for hebrew_char in '××‘×’×“×”×•×–×—×˜×™×›×œ×× ×¡×¢×¤×¦×§×¨×©×ª')
                        
                        if has_hebrew or 'he' in languages or 'iw' in languages:
                            continue
                        
                        if not ('en' in languages or 'en-US' in languages or 'en-GB' in languages):
                            if not languages:
                                if has_hebrew:
                                    continue
                            else:
                                continue
                    
                    # ×‘×“×™×§×ª ××©×š ×× × ×“×¨×©
                    duration_minutes = self.get_episode_duration(show['id'])
                    if max_duration and duration_minutes and duration_minutes > max_duration:
                        continue
                    
                    # ×”×•×¡×¤×” ×œ×ª×•×¦××•×ª ×× ×œ× ×§×™×™× ×›×‘×¨
                    if not any(r['name'] == show['name'] for r in all_results):
                        all_results.append({
                            'name': show['name'],
                            'publisher': show['publisher'],
                            'description': show['description'][:200] + "..." if len(show['description']) > 200 else show['description'],
                            'url': show['external_urls']['spotify'],
                            'duration_minutes': duration_minutes,
                            'languages': languages,
                            'total_episodes': show.get('total_episodes', 0)
                        })
                        
            except Exception as e:
                print(f"Error searching Spotify: {e}")
                continue
        
        return all_results[:limit]

    def get_episode_duration(self, show_id):
        """×§×‘×œ×ª ××©×š ×”×¤×¨×§ ×”××—×¨×•×Ÿ"""
        headers = {'Authorization': f'Bearer {self.token}'}
        try:
            response = requests.get(
                f"https://api.spotify.com/v1/shows/{show_id}/episodes?limit=1",
                headers=headers,
                timeout=10
            )
            if response.status_code == 200:
                items = response.json().get('items', [])
                if items:
                    duration_ms = items[0].get('duration_ms', 0)
                    return round(duration_ms / 60000, 1) if duration_ms > 0 else None
        except:
            pass
        return None

class GPTAnalyzer:
    def __init__(self):
        self.conversation_history = {}
    
    async def check_podcast_relevance(self, user_text):
        """×‘×“×™×§×” ×× ×”×‘×§×©×” ×¨×œ×•×•× ×˜×™×ª ×œ×¤×•×“×§××¡×˜×™× - ×‘×“×™×§×” ×™×“× ×™×ª ×§×©×™×—×” ×§×•×“×"""
        
        # ×‘×“×™×§×” ×™×“× ×™×ª ×§×©×™×—×” ×§×•×“× ×›×œ
        manual_check = self.strict_manual_check(user_text)
        if manual_check["confidence"] > 0.8:
            print(f"ğŸ” Manual strict check for '{user_text}': {manual_check}")
            return manual_check
        
        # ×¨×§ ×× ×”×‘×“×™×§×” ×”×™×“× ×™×ª ×œ× ×•×“××™×ª, × × ×¡×” GPT
        system_prompt = """××ª×” ××¡×•×•×’ ×”×•×“×¢×•×ª ×©×œ ××©×ª××©×™×. 
        
        ×”×¦'×˜×‘×•×˜ ×©×œ× ×• ××™×•×¢×“ ×œ×”××œ×™×¥ ×¢×œ ×¤×•×“×§××¡×˜×™× ×‘×œ×‘×“.
        
        ×”×—×–×¨ JSON ×¢× ×”×©×“×” ×”×‘×:
        {
            "is_podcast_related": true/false,
            "confidence": 0.0-1.0,
            "reason": "×”×¡×‘×¨ ×§×¦×¨ ×œ××” ×–×” ×§×©×•×¨ ××• ×œ× ×§×©×•×¨ ×œ×¤×•×“×§××¡×˜×™×"
        }
        
        ×“×•×’×××•×ª ×œ×‘×§×©×•×ª ×©×§×©×•×¨×•×ª ×œ×¤×•×“×§××¡×˜×™× (is_podcast_related: true):
        - "×¨×•×¦×” ×¤×•×“×§××¡×˜ ×¢×œ ×¡×¤×•×¨×˜"
        - "××©×”×• ×œ×©××•×¢ ×¢×œ ×˜×›× ×•×œ×•×’×™×”"
        - "×ª×›× ×™×ª ×¨×“×™×• ×¢×œ ×—×“×©×•×ª"
        - "××™×–×” ×ª×•×›×Ÿ ××•×“×™×• ×¢×œ ×‘×™×©×•×œ"
        - "×©×™×—×•×ª ×¢×œ ×¤×¡×™×›×•×œ×•×’×™×”"
        - "×”××œ×¦×” ×œ×ª×•×›× ×™×ª ×¢×œ ×¢×¡×§×™×"
        
        ×“×•×’×××•×ª ×œ×‘×§×©×•×ª ×©×œ× ×§×©×•×¨×•×ª ×œ×¤×•×“×§××¡×˜×™× (is_podcast_related: false):
        - "××” ××–×’ ×”××•×•×™×¨?"
        - "××™×š ×§×•×¨××™× ×œ×š?"
        - "××ª×™ ×”×¤×¡×—×?"
        - "×”×™×™"
        - "×ª×•×“×”"
        - "××” ×”×©×¢×”?"
        - "××™×š ××’×™×¢×™× ×œ×ª×œ ××‘×™×‘?"
        
        ×—×©×•×‘: ×’× ×× ×”××©×ª××© ×›×•×ª×‘ × ×•×©× ×›×œ×œ×™ ×›××• "×¡×¤×•×¨×˜" ××• "×˜×›× ×•×œ×•×’×™×”" - ×× ×–×” ×™×›×•×œ ×œ×”×™×•×ª ×‘×§×©×” ×œ×¤×•×“×§××¡×˜, ×”×—×–×¨ true."""

        try:
            response = await openai.ChatCompletion.acreate(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"×‘×“×•×§ ×× ×”×‘×§×©×” ×”×–×• ×§×©×•×¨×” ×œ×¤×•×“×§××¡×˜×™×: '{user_text}'"}
                ],
                temperature=0.1,
                max_tokens=200
            )
            
            result = json.loads(response.choices[0].message.content)
            print(f"ğŸ” GPT check for '{user_text}': {result}")
            return result
            
        except Exception as e:
            print(f"Error in podcast relevance check: {e}")
            return manual_check
    
    def strict_manual_check(self, user_text):
        """×‘×“×™×§×” ×™×“× ×™×ª ×§×©×™×—×” ×××•×“ ×œ×‘×§×©×•×ª ×©×œ× ×§×©×•×¨×•×ª ×œ×¤×•×“×§××¡×˜×™×"""
        text_lower = user_text.lower().strip()
        
        # ×¨×©×™××” ××•×¨×—×‘×ª ×©×œ ×‘×™×˜×•×™×™× ×©×‘×•×•×“××•×ª ×œ× ×§×©×•×¨×™× ×œ×¤×•×“×§××¡×˜×™×
        definitely_not_podcast = [
            # ××–×’ ××•×•×™×¨
            '××” ××–×’ ×”××•×•×™×¨', '××–×’ ×”××•×•×™×¨', 'weather', '×˜××¤×¨×˜×•×¨×”', 
            '×’×©×', '×©××©', '×©×œ×’', '×¨×•×—', '×¢× × ×™×', '×—×', '×§×¨',
            
            # ×–××Ÿ ×•×ª××¨×™×›×™×
            '××” ×”×©×¢×”', '××™×–×” ×©×¢×”', '××” ×”×–××Ÿ', '××ª×™', '×ª××¨×™×š',
            '×™×•×', '×—×•×“×©', '×©× ×”', '××—×¨', '××ª××•×œ',
            
            # ×‘×¨×›×•×ª ×•×—×‘×¨×•×ª×™×•×ª
            '×”×™×™', '×©×œ×•×', 'hello', 'hi', '×‘×•×§×¨ ×˜×•×‘', '×œ×™×œ×” ×˜×•×‘',
            '××” ×©×œ×•××š', '××™×š ×”×•×œ×š', '××” × ×©××¢',
            
            # ×ª×•×“×•×ª
            '×ª×•×“×”', 'thanks', '×ª×•×“×” ×¨×‘×”', '××™×Ÿ ×‘×¢×“ ××”',
            
            # ×©××œ×•×ª ×–×”×•×ª
            '××™×š ×§×•×¨××™× ×œ×š', '××™ ××ª×”', '××” ×”×©× ×©×œ×š', '××” ××ª×”',
            
            # × ×™×•×•×˜ ×•××™×§×•×
            '××™×š ××’×™×¢×™×', '××™×¤×” × ××¦×', '×“×¨×š', '× ×¡×™×¢×”', '×›×ª×•×‘×ª',
            
            # ×—×’×™× ×•××™×¨×•×¢×™×
            '××ª×™ ×¤×¡×—', '××ª×™ ×¨××© ×”×©× ×”', '×—×’', '×—×’×™×',
            
            # ×©××œ×•×ª ×›×œ×œ×™×•×ª
            '×œ××”', '××™×š', '××ª×™', '××™×¤×”', '××™', '××” ×–×”',
            
            # ×‘×¢×™×•×ª ×˜×›× ×™×•×ª
            '×œ× ×¢×•×‘×“', '×©×’×™××”', '×‘×¢×™×”', '×ª×§×œ×”'
        ]
        
        # ×‘×“×™×§×” ×× ×™×© ×”×ª×××” ××“×•×™×§×ª
        for phrase in definitely_not_podcast:
            if phrase in text_lower:
                return {
                    "is_podcast_related": False,
                    "confidence": 0.95,
                    "reason": f"××›×™×œ ×‘×™×˜×•×™ ×©×œ× ×§×©×•×¨ ×œ×¤×•×“×§××¡×˜×™×: '{phrase}'"
                }
        
        # ×‘×“×™×§×” ×× ×–×” ×¨×§ ×©××œ×” ×§×¦×¨×” (×¤×—×•×ª ×-4 ××™×œ×™×) ×œ×œ× ××™×œ×•×ª ××¤×ª×— ×©×œ ×¤×•×“×§××¡×˜×™×
        words = text_lower.split()
        if len(words) <= 3:
            podcast_keywords = [
                '×¤×•×“×§××¡×˜', 'podcast', '×œ×©××•×¢', '×”××–× ×”', '×ª×•×›× ×™×ª',
                '×©×™×—×•×ª', '×¨××™×•× ×•×ª', '×ª×•×›×Ÿ', '××•×“×™×•', '×¨×“×™×•', '×¢× ×™×™×Ÿ'
            ]
            
            has_podcast_keyword = any(keyword in text_lower for keyword in podcast_keywords)
            if not has_podcast_keyword:
                return {
                    "is_podcast_related": False,
                    "confidence": 0.85,
                    "reason": "×©××œ×” ×§×¦×¨×” ×œ×œ× ××™×œ×•×ª ××¤×ª×— ×©×œ ×¤×•×“×§××¡×˜×™×"
                }
        
        # ×× ×œ× ××¦×× ×• ×¡×™×‘×” ×‘×¨×•×¨×” ×œ×“×—×•×ª, × ×—×–×™×¨ ×‘×™×˜×—×•×Ÿ × ××•×š
        return {
            "is_podcast_related": True,
            "confidence": 0.3,
            "reason": "×œ× × ××¦××” ×¡×™×‘×” ×‘×¨×•×¨×” ×œ×“×—×™×”"
        }
    
    def manual_relevance_check(self, user_text):
        """×‘×“×™×§×” ×™×“× ×™×ª ×›××©×¨ GPT ×œ× ×¢×•×‘×“"""
        text_lower = user_text.lower()
        
        # ××™×œ×™× ×©×‘×¨×•×¨ ×©×œ× ×§×©×•×¨×•×ª ×œ×¤×•×“×§××¡×˜×™×
        non_podcast_keywords = [
            '××–×’ ×”××•×•×™×¨', 'weather', '×˜××¤×¨×˜×•×¨×”', '×’×©×', '×©××©',
            '××” ×”×©×¢×”', '×–××Ÿ', '×ª××¨×™×š', '×™×•×',
            '×”×™×™', '×©×œ×•×', 'hello', 'hi',
            '×ª×•×“×”', 'thanks', '×‘×•×§×¨ ×˜×•×‘', '×œ×™×œ×” ×˜×•×‘',
            '××™×š ×§×•×¨××™× ×œ×š', '××™ ××ª×”', '××” ×–×”',
            '××™×š ××’×™×¢×™×', '× ×¡×™×¢×”', '×“×¨×š', '××™×§×•×'
        ]
        
        # ×× ×™×© ××™×œ×ª ××¤×ª×— ×‘×¨×•×¨×” ×©×œ× ×§×©×•×¨×” ×œ×¤×•×“×§××¡×˜×™×
        if any(keyword in text_lower for keyword in non_podcast_keywords):
            return {
                "is_podcast_related": False,
                "confidence": 0.9,
                "reason": "××›×™×œ ××™×œ×•×ª ××¤×ª×— ×©×œ× ×§×©×•×¨×•×ª ×œ×¤×•×“×§××¡×˜×™×"
            }
        
        # ××™×œ×™× ×©××¨××–×•×ª ×¢×œ ×¤×•×“×§××¡×˜×™×
        podcast_keywords = [
            '×¤×•×“×§××¡×˜', 'podcast', '×œ×©××•×¢', '×”××–× ×”', '×ª×•×›× ×™×ª',
            '×©×™×—×•×ª', '×¨××™×•× ×•×ª', '×ª×•×›×Ÿ ××•×“×™×•', '×¨×“×™×•'
        ]
        
        # ×× ×™×© ××™×œ×ª ××¤×ª×— ×‘×¨×•×¨×” ×œ×¤×•×“×§××¡×˜×™×
        if any(keyword in text_lower for keyword in podcast_keywords):
            return {
                "is_podcast_related": True,
                "confidence": 0.9,
                "reason": "××›×™×œ ××™×œ×•×ª ××¤×ª×— ×©×œ ×¤×•×“×§××¡×˜×™×"
            }
        
        # ×‘××§×¨×” ×©×œ ×¡×¤×§ - × × ×™×— ×©×–×” ×œ× ×§×©×•×¨ ×œ×¤×•×“×§××¡×˜×™×
        return {
            "is_podcast_related": False,
            "confidence": 0.6,
            "reason": "×œ× ××–×•×”×” ×›×‘×§×©×” ×œ×¤×•×“×§××¡×˜"
        }
    
    async def analyze_request(self, user_text, user_id):
        """× ×™×ª×•×— ×‘×§×©×ª ×”××©×ª××© ×¢× GPT - ×›×•×œ×œ ×‘×“×™×§×ª ×¨×œ×•×•× ×˜×™×•×ª"""
        
        # ×§×•×“× ×‘×•×“×§×™× ×× ×–×” ×§×©×•×¨ ×œ×¤×•×“×§××¡×˜×™×
        relevance_check = await self.check_podcast_relevance(user_text)
        
        # ×× ×–×” ×œ× ×§×©×•×¨ ×œ×¤×•×“×§××¡×˜×™× ×¢× ×‘×™×˜×—×•×Ÿ ×’×‘×•×”
        if not relevance_check["is_podcast_related"] and relevance_check["confidence"] > 0.7:
            return {
                "is_podcast_related": False,
                "reason": relevance_check["reason"],
                "suggested_response": self.generate_non_podcast_response(user_text)
            }
        
        # ×× ×–×” ×§×©×•×¨ ×œ×¤×•×“×§××¡×˜×™×, ×××©×™×›×™× ×œ× ×™×ª×•×— ×¨×’×™×œ
        history = self.conversation_history.get(user_id, [])
        context = ""
        if history:
            context = f"×”×™×¡×˜×•×¨×™×™×ª ×©×™×—×” ×§×•×“××ª: {history[-2:]}"
        
        system_prompt = f"""××ª×” ×¢×•×–×¨ ×”××œ×¦×•×ª ×¤×•×“×§××¡×˜×™×. × ×ª×— ××ª ×”×‘×§×©×” ×•×”×—×–×¨ JSON ×¢× ×”××‘× ×” ×”×‘×:

{context}

{{
    "topics": ["×¨×©×™××ª × ×•×©××™× ×©××¢× ×™×™× ×™× ××ª ×”××©×ª××©"],
    "duration_max": ××¡×¤×¨ ×“×§×•×ª ××§×¡×™××œ×™ ××• null,
    "language_preference": "hebrew" ××• "english" ××• null,
    "keywords": ["××™×œ×•×ª ××¤×ª×— ×œ×—×™×¤×•×©"],
    "user_intent": "×ª×™××•×¨ ×›×•×•× ×ª ×”××©×ª××©",
    "is_podcast_related": true
}}

×—×©×•×‘:
- ×× ×”××©×ª××© ××‘×§×© "×‘×× ×’×œ×™×ª" ××• "english" - language_preference: "english"
- ×× ×”××©×ª××© ××‘×§×© "×‘×¢×‘×¨×™×ª" ××• ×œ× ××¦×™×™×Ÿ ×©×¤×” - language_preference: "hebrew"
- ×× ×”××©×ª××© ××¦×™×™×Ÿ ×–××Ÿ (10 ×“×§×•×ª, 5 ×“×§×•×ª) - duration_max: ×”××¡×¤×¨"""

        try:
            response = await openai.ChatCompletion.acreate(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_text}
                ],
                temperature=0.7,
                max_tokens=300
            )
            
            result = json.loads(response.choices[0].message.content)
            result["is_podcast_related"] = True
            
            # ×©××™×¨×ª ×”××™× ×˜×¨××§×¦×™×”
            if user_id not in self.conversation_history:
                self.conversation_history[user_id] = []
            self.conversation_history[user_id].append({
                'user_input': user_text,
                'analysis': result
            })
            
            # ×©××™×¨×” ×¢×œ ×”×™×¡×˜×•×¨×™×” ××•×’×‘×œ×ª
            if len(self.conversation_history[user_id]) > 5:
                self.conversation_history[user_id] = self.conversation_history[user_id][-5:]
            
            return result
            
        except Exception as e:
            print(f"GPT Analysis error: {e}")
            basic_result = self.basic_analysis(user_text)
            basic_result["is_podcast_related"] = True
            return basic_result
    
    def generate_non_podcast_response(self, user_text):
        """×™×•×¦×¨ ×ª×’×•×‘×” ××ª××™××” ×œ×‘×§×©×•×ª ×©×œ× ×§×©×•×¨×•×ª ×œ×¤×•×“×§××¡×˜×™×"""
        
        text_lower = user_text.lower()
        
        if any(word in text_lower for word in ['××–×’', 'weather', '×˜××¤×¨×˜×•×¨×”']):
            return "ğŸŒ¤ï¸ ×× ×™ ××ª××—×” ×¨×§ ×‘××ª×Ÿ ×”××œ×¦×•×ª ×¢×œ ×¤×•×“×§××¡×˜×™×, ×œ× ×‘××–×’ ×”××•×•×™×¨.\n\n×× ××ª×” ××—×¤×© ×¤×•×“×§××¡×˜ ×¢×œ ××˜××•×¨×•×œ×•×’×™×” ××• ××“×¢×™ ×”××˜××•×¡×¤×™×¨×” - ×× ×™ ××©××— ×œ×¢×–×•×¨! ğŸ§"
        
        elif any(word in text_lower for word in ['×©×¢×”', '×–××Ÿ', '×ª××¨×™×š']):
            return "â° ×× ×™ ×œ× ×™×•×“×¢ ××” ×”×©×¢×”, ××‘×œ ×× ×™ ×™×›×•×œ ×œ×”××œ×™×¥ ×œ×š ×¢×œ ×¤×•×“×§××¡×˜×™× ××¢×•×œ×™×!\n\n××” ××¢× ×™×™×Ÿ ××•×ª×š ×œ×©××•×¢? ğŸ§"
        
        elif any(word in text_lower for word in ['×”×™×™', '×©×œ×•×', 'hello']):
            return "ğŸ‘‹ ×©×œ×•×! ×× ×™ SHMALI - ×”×‘×•×˜ ×œ×”××œ×¦×•×ª ×¤×•×“×§××¡×˜×™×!\n\n×¡×¤×¨ ×œ×™ ××™×–×” × ×•×©× ××¢× ×™×™×Ÿ ××•×ª×š ×•×××¦× ×œ×š ×¤×•×“×§××¡×˜ ××•×©×œ×! ğŸ§"
        
        elif any(word in text_lower for word in ['×ª×•×“×”', 'thanks']):
            return "ğŸ˜Š ××™×Ÿ ×‘×¢×“ ××”! ×”×× ×ª×¨×¦×” ×¢×•×“ ×”××œ×¦×•×ª ×¢×œ ×¤×•×“×§××¡×˜×™×? ×¤×©×•×˜ ×¡×¤×¨ ×œ×™ ××™×–×” × ×•×©× ××¢× ×™×™×Ÿ ××•×ª×š! ğŸ§"
        
        elif any(word in text_lower for word in ['×“×¨×š', '× ×¡×™×¢×”', '××™×§×•×']):
            return "ğŸ—ºï¸ ×× ×™ ×œ× ××•××—×” ×‘× ×™×•×•×˜, ××‘×œ ×× ×™ ×™×›×•×œ ×œ×”××œ×™×¥ ×œ×š ×¢×œ ×¤×•×“×§××¡×˜×™× × ×”×“×¨×™× ×œ×“×¨×š!\n\n××™×–×” × ×•×©× ×ª×¨×¦×” ×œ×©××•×¢ ×‘× ×¡×™×¢×”? ğŸ§"
        
        else:
            return f"ğŸ¤– ×× ×™ ××ª××—×” ×¨×§ ×‘××ª×Ÿ ×”××œ×¦×•×ª ×¢×œ ×¤×•×“×§××¡×˜×™×.\n\n×× '{user_text}' ×§×©×•×¨ ×œ× ×•×©× ×©×ª×¨×¦×” ×œ×©××•×¢ ×¢×œ×™×• ×‘×¤×•×“×§××¡×˜ - ×¡×¤×¨ ×œ×™ ×™×•×ª×¨ ×¤×¨×˜×™×! ğŸ§"
    
    def basic_analysis(self, text):
        """× ×™×ª×•×— ×‘×¡×™×¡×™ ×× GPT × ×›×©×œ"""
        text_lower = text.lower()
        
        # ×–×™×”×•×™ ×©×¤×”
        language_preference = None
        if any(word in text_lower for word in ['×‘×× ×’×œ×™×ª', '×× ×’×œ×™×ª', 'english', 'in english']):
            language_preference = 'english'
        elif any(word in text_lower for word in ['×‘×¢×‘×¨×™×ª', '×¢×‘×¨×™×ª']):
            language_preference = 'hebrew'
        else:
            language_preference = 'hebrew'
        
        # ×–×™×”×•×™ ××©×š ×–××Ÿ
        duration_match = re.search(r'(\d+)\s*×“×§', text)
        duration_max = int(duration_match.group(1)) if duration_match else None
        
        # ×–×™×”×•×™ × ×•×©××™×
        topics = []
        topic_keywords = {
            '×¡×¤×•×¨×˜': ['×¡×¤×•×¨×˜', '×›×“×•×¨×’×œ', '×›×“×•×¨×¡×œ', '××™××•×Ÿ'],
            '×˜×›× ×•×œ×•×’×™×”': ['×˜×›× ×•×œ×•×’×™×”', '××—×©×‘', '×ª×›× ×•×ª', '××¤×œ×™×§×¦×™×”'],
            '×‘×¨×™××•×ª': ['×‘×¨×™××•×ª', '×ª×–×•× ×”', '×“×™××˜×”', '×¨×¤×•××”'],
            '×§×•××“×™×”': ['×§×•××“×™×”', '××¦×—×™×§', '×”×•××•×¨', '×¦×—×•×§'],
            '×—×“×©×•×ª': ['×—×“×©×•×ª', '×¤×•×œ×™×˜×™×§×”', '××§×˜×•××œ×™×”'],
            '×¢×¡×§×™×': ['×¢×¡×§×™×', '×›×¡×£', '×™×–××•×ª', '×”×©×§×¢×•×ª']
        }
        
        for topic, keywords in topic_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                topics.append(topic)
        
        return {
            "topics": topics,
            "duration_max": duration_max,
            "language_preference": language_preference,
            "keywords": text_lower.split()[:3],
            "user_intent": text[:100]
        }

class SimilarityScorer:
    """××—×œ×§×” ×œ×—×™×©×•×‘ ×¦×™×•×Ÿ ×“××™×•×Ÿ 70%-30%"""
    
    @staticmethod
    def calculate_text_similarity(text1, text2):
        """×—×™×©×•×‘ ×“××™×•×Ÿ ×˜×§×¡×˜×•××œ×™ ×‘×™×Ÿ ×©× ×™ ×˜×§×¡×˜×™×"""
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
    
    @staticmethod
    def calculate_topic_similarity(user_analysis, podcast_data):
        """×—×™×©×•×‘ ×”×ª×××” ×œ× ×•×©××™× ×¢×™×§×¨×™×™× (70% ××”×¦×™×•×Ÿ)"""
        user_topics = user_analysis.get('topics', [])
        user_keywords = user_analysis.get('keywords', [])
        
        podcast_name = podcast_data.get('name', '').lower()
        podcast_description = podcast_data.get('description', '').lower()
        podcast_publisher = podcast_data.get('publisher', '').lower()
        
        # ×—×™×¤×•×© ×”×ª×××•×ª ×™×©×™×¨×•×ª
        direct_matches = 0
        total_terms = len(user_topics) + len(user_keywords)
        
        if total_terms == 0:
            return 0.5  # ×¦×™×•×Ÿ × ×™×™×˜×¨×œ×™ ×× ××™×Ÿ × ×•×©××™×
        
        # ×‘×“×™×§×ª × ×•×©××™×
        for topic in user_topics:
            topic_lower = topic.lower()
            if (topic_lower in podcast_name or 
                topic_lower in podcast_description or 
                topic_lower in podcast_publisher):
                direct_matches += 2  # × ×•×©××™× ××§×‘×œ×™× ×¦×™×•×Ÿ ×›×¤×•×œ
        
        # ×‘×“×™×§×ª ××™×œ×•×ª ××¤×ª×—
        for keyword in user_keywords:
            keyword_lower = keyword.lower()
            if (keyword_lower in podcast_name or 
                keyword_lower in podcast_description or 
                keyword_lower in podcast_publisher):
                direct_matches += 1
        
        # ×—×™×©×•×‘ ×¦×™×•×Ÿ ×“××™×•×Ÿ ×˜×§×¡×˜×•××œ×™
        name_similarity = SimilarityScorer.calculate_text_similarity(
            ' '.join(user_topics + user_keywords), 
            podcast_name
        )
        description_similarity = SimilarityScorer.calculate_text_similarity(
            ' '.join(user_topics + user_keywords), 
            podcast_description
        )
        
        # ××©×§×œ ×œ×—×™×©×•×‘ ×”×¡×•×¤×™
        direct_score = min(direct_matches / (total_terms * 2), 1.0)  # × ×¨××•×œ ×œ-1
        similarity_score = max(name_similarity, description_similarity)
        
        # ×¦×™×•×Ÿ ×¡×•×¤×™ ×œ× ×•×©××™×
        topic_score = (direct_score * 0.7) + (similarity_score * 0.3)
        
        return min(topic_score, 1.0)
    
    @staticmethod
    def calculate_metadata_similarity(user_analysis, podcast_data):
        """×—×™×©×•×‘ ×”×ª×××” ×œ××˜××“×˜×” (30% ××”×¦×™×•×Ÿ)"""
        metadata_score = 0.0
        checks = 0
        
        # ×‘×“×™×§×ª ×©×¤×” (50% ××”××˜××“×˜×”)
        user_language = user_analysis.get('language_preference')
        podcast_languages = podcast_data.get('languages', [])
        
        if user_language:
            checks += 1
            if user_language == 'hebrew' and ('he' in podcast_languages or 'iw' in podcast_languages):
                metadata_score += 0.5
            elif user_language == 'english' and any('en' in lang for lang in podcast_languages):
                metadata_score += 0.5
            # ×× ××™×Ÿ ×”×ª×××” ×‘×©×¤×”, ×œ× ××•×¡×™×¤×™× ×¦×™×•×Ÿ
        
        # ×‘×“×™×§×ª ××©×š ×–××Ÿ (30% ××”××˜××“×˜×”)
        user_max_duration = user_analysis.get('duration_max')
        podcast_duration = podcast_data.get('duration_minutes')
        
        if user_max_duration and podcast_duration:
            checks += 1
            if podcast_duration <= user_max_duration:
                # ×¦×™×•×Ÿ ×’×‘×•×” ×™×•×ª×¨ ×œ×¤×•×“×§××¡×˜×™× ×§×¦×¨×™× ×™×•×ª×¨ (×™×•×ª×¨ ×˜×•×‘)
                duration_ratio = 1 - (podcast_duration / user_max_duration)
                metadata_score += 0.3 * (0.5 + 0.5 * duration_ratio)
            # ×× ×”×¤×•×“×§××¡×˜ ××¨×•×š ××“×™, ×œ× ××•×¡×™×¤×™× ×¦×™×•×Ÿ
        
        # ×‘×“×™×§×ª ×›××•×ª ×¤×¨×§×™× (20% ××”××˜××“×˜×”)
        total_episodes = podcast_data.get('total_episodes', 0)
        if total_episodes > 0:
            checks += 1
            # ×”×¢×“×¤×” ×œ×¤×•×“×§××¡×˜×™× ×¢× ×›××•×ª ×¡×‘×™×¨×” ×©×œ ×¤×¨×§×™×
            if 5 <= total_episodes <= 100:
                metadata_score += 0.2
            elif total_episodes > 100:
                metadata_score += 0.1  # ×¦×™×•×Ÿ × ××•×š ×™×•×ª×¨ ×œ×¤×•×“×§××¡×˜×™× ×¢× ×”×¨×‘×” ×××•×“ ×¤×¨×§×™×
        
        # ×× ×œ× ×”×™×• ×‘×“×™×§×•×ª, ××—×–×™×¨×™× ×¦×™×•×Ÿ × ×™×™×˜×¨×œ×™
        return metadata_score if checks > 0 else 0.5
    
    @staticmethod
    def calculate_similarity_score(user_analysis, podcast_data):
        """×—×™×©×•×‘ ×¦×™×•×Ÿ ×“××™×•×Ÿ ××¡×•×¤×™ (70% × ×•×©××™× + 30% ××˜××“×˜×”)"""
        
        # 70% - ×”×ª×××” ×œ× ×•×©××™× ×¢×™×§×¨×™×™×
        topic_score = SimilarityScorer.calculate_topic_similarity(user_analysis, podcast_data)
        
        # 30% - ×”×ª×××” ×œ××˜××“×˜×”
        metadata_score = SimilarityScorer.calculate_metadata_similarity(user_analysis, podcast_data)
        
        # ×¦×™×•×Ÿ ××¡×•×¤×™
        final_score = (topic_score * 0.7) + (metadata_score * 0.3)
        
        return round(final_score, 3)

class ShmaliBot:
    def __init__(self):
        self.spotify = SpotifyAPI(CLIENT_ID, CLIENT_SECRET)
        self.gpt_analyzer = GPTAnalyzer()
        self.similarity_scorer = SimilarityScorer()
        self.load_local_data()
        self.shown_recommendations = {}
        self.available_recommendations = {}
    
    def load_local_data(self):
        """×˜×¢×™× ×ª × ×ª×•× ×™× ××§×•××™×™×"""
        try:
            self.df = pd.read_csv(DATASET_CSV)
            print(f"âœ… × ×˜×¢× ×• {len(self.df)} ×¤×•×“×§××¡×˜×™× ××§×•××™×™×")
        except FileNotFoundError:
            print("âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ × ×ª×•× ×™× ××§×•××™")
            self.df = pd.DataFrame()
    
    def search_local_dataset(self, analysis):
        """×—×™×¤×•×© ×‘×“×˜×”×¡×˜ ×”××§×•××™ ×¢× ×“×™×¨×•×’ similarity"""
        if self.df.empty:
            return []
        
        local_results = []
        topics = analysis.get('topics', [])
        keywords = analysis.get('keywords', [])
        language_pref = analysis.get('language_preference')
        max_duration = analysis.get('duration_max')
        
        for _, podcast in self.df.iterrows():
            # ×”××¨×” ×œ×¤×•×¨××˜ ×¡×˜× ×“×¨×˜×™
            podcast_data = {
                'name': podcast.get('name', 'Unknown'),
                'publisher': podcast.get('publisher', 'Unknown'),
                'description': str(podcast.get('description', '')),
                'url': podcast.get('url', '#'),
                'duration_minutes': podcast.get('duration_minutes'),
                'languages': [podcast.get('language', '')] if pd.notna(podcast.get('language')) else [],
                'total_episodes': podcast.get('total_episodes', 0),
                'source': 'local_dataset'
            }
            
            # ×—×™×©×•×‘ ×¦×™×•×Ÿ ×“××™×•×Ÿ
            similarity_score = self.similarity_scorer.calculate_similarity_score(analysis, podcast_data)
            
            # ×¡×£ ××™× ×™××œ×™ ×œ×§×‘×œ×ª ×¤×•×“×§××¡×˜
            if similarity_score >= 0.3:
                podcast_data['similarity_score'] = similarity_score
                local_results.append(podcast_data)
        
        # ××™×•×Ÿ ×œ×¤×™ ×¦×™×•×Ÿ ×“××™×•×Ÿ
        local_results.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        return local_results
    
    async def get_recommendations(self, analysis, user_id):
        """×§×‘×œ×ª ×”××œ×¦×•×ª ××©×™×œ×•×‘ ×©×œ Spotify ×•×”×“×˜×”×¡×˜ ×”××§×•××™ ×¢× ×“×™×¨×•×’ similarity"""
        
        # Debug - ××” ×™×© ×œ× ×•
        print(f"ğŸ” Getting recommendations for user {user_id}")
        print(f"   Topics: {analysis.get('topics', [])}")
        print(f"   Keywords: {analysis.get('keywords', [])}")
        
        # ×× ×–×• ×‘×§×©×” ×¨××©×•× ×” ××• ×©×”××©×ª××© ×‘×™×§×© × ×•×©× ×—×“×©, × ××¤×¡ ××ª ×”×¨×©×™××•×ª
        if user_id not in self.available_recommendations or self.is_new_topic(analysis, user_id):
            print(f"ğŸ”„ Resetting recommendations for user {user_id} (new topic or first time)")
            
            self.shown_recommendations[user_id] = []
            self.available_recommendations[user_id] = []
            
            all_recommendations = []
            
            # ×—×™×¤×•×© ×‘×¡×¤×•×˜×™×¤×™×™ ×§×•×“×
            topics = analysis.get('topics', [])
            if topics:
                print(f"ğŸµ Searching Spotify for topics: {topics}")
                for topic in topics:
                    try:
                        spotify_results = self.spotify.search_podcasts(
                            query=topic,
                            max_duration=analysis.get('duration_max'),
                            language_preference=analysis.get('language_preference'),
                            limit=10
                        )
                        print(f"   Found {len(spotify_results)} results for topic '{topic}'")
                        for result in spotify_results:
                            result['source'] = 'spotify'
                            # ×—×™×©×•×‘ ×¦×™×•×Ÿ ×“××™×•×Ÿ ×œ×ª×•×¦××•×ª Spotify
                            result['similarity_score'] = self.similarity_scorer.calculate_similarity_score(analysis, result)
                        all_recommendations.extend(spotify_results)
                    except Exception as e:
                        print(f"âŒ Error searching Spotify for topic '{topic}': {e}")
            
            # ×× ××™×Ÿ × ×•×©××™× ×¡×¤×¦×™×¤×™×™×, ×—×¤×© ×œ×¤×™ ××™×œ×•×ª ××¤×ª×—
            if len(all_recommendations) < 5 and analysis.get('keywords'):
                print(f"ğŸ” Searching by keywords: {analysis['keywords']}")
                query = ' '.join(analysis['keywords'][:2])
                try:
                    spotify_results = self.spotify.search_podcasts(
                        query=query,
                        max_duration=analysis.get('duration_max'),
                        language_preference=analysis.get('language_preference'),
                        limit=10
                    )
                    print(f"   Found {len(spotify_results)} results for keywords")
                    for result in spotify_results:
                        result['source'] = 'spotify'
                        result['similarity_score'] = self.similarity_scorer.calculate_similarity_score(analysis, result)
                    all_recommendations.extend(spotify_results)
                except Exception as e:
                    print(f"âŒ Error searching Spotify by keywords: {e}")
            
            # ×—×™×¤×•×© ×‘×“×˜×”×¡×˜ ×”××§×•××™ ××—×¨×™ Spotify
            print(f"ğŸ“ Searching local dataset...")
            try:
                local_results = self.search_local_dataset(analysis)
                print(f"   Found {len(local_results)} local results")
                all_recommendations.extend(local_results)
            except Exception as e:
                print(f"âŒ Error searching local dataset: {e}")
            
            # ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª
            seen = set()
            unique_recommendations = []
            for rec in all_recommendations:
                if rec['name'] not in seen:
                    seen.add(rec['name'])
                    unique_recommendations.append(rec)
            
            print(f"ğŸ“Š Total unique recommendations: {len(unique_recommendations)}")
            
            # ××™×•×Ÿ ×œ×¤×™ ×¦×™×•×Ÿ ×“××™×•×Ÿ ×‘××§×•× ×¢×¨×‘×•×‘ ×¨× ×“×•××œ×™
            unique_recommendations.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
            
            # ×©××™×¨×” ×¢× ×œ×•×’ ×¦×™×•× ×™×
            print(f"ğŸ“Š Top recommendations for user {user_id}:")
            for i, rec in enumerate(unique_recommendations[:5]):
                print(f"  {i+1}. {rec['name']} - Score: {rec.get('similarity_score', 'N/A')}")
            
            self.available_recommendations[user_id] = unique_recommendations
        else:
            print(f"â™»ï¸ Using existing recommendations for user {user_id}")
        
        # ××—×–×™×¨×™× ×”××œ×¦×” ××—×ª ×©×¢×•×“ ×œ× ×”×•×¦×’×”
        available_recs = self.available_recommendations.get(user_id, [])
        shown_recs = self.shown_recommendations.get(user_id, [])
        
        print(f"ğŸ“ Available: {len(available_recs)}, Shown: {len(shown_recs)}")
        
        for rec in available_recs:
            if rec['name'] not in shown_recs:
                self.shown_recommendations[user_id].append(rec['name'])
                print(f"âœ… Returning recommendation: {rec['name']}")
                return [rec]
        
        # ×× × ×’××¨×• ×”×”××œ×¦×•×ª
        print(f"âŒ No more recommendations available for user {user_id}")
        return []
    
    def is_new_topic(self, analysis, user_id):
        """×‘×•×“×§ ×× ×”××©×ª××© ××‘×§×© × ×•×©× ×—×“×©"""
        if user_id not in self.gpt_analyzer.conversation_history:
            return True
        
        # ××©×•×•×™× ××ª ×”× ×•×©××™× ×”× ×•×›×—×™×™× ×œ×§×•×“××™×
        history = self.gpt_analyzer.conversation_history.get(user_id, [])
        if len(history) < 2:
            return False
        
        current_topics = set(analysis.get('topics', []))
        previous_topics = set(history[-2]['analysis'].get('topics', []))
        
        return current_topics != previous_topics

def create_personalized_intro(analysis, is_more_request):
    """×™×¦×™×¨×ª ×”×§×“××” ××™×©×™×ª ×¢×œ ×‘×¡×™×¡ ×”× ×™×ª×•×—"""
    topics = analysis.get('topics', [])
    duration = analysis.get('duration_max')
    language = analysis.get('language_preference')
    
    if is_more_request:
        intro = "×”× ×” ×¢×•×“ ×”××œ×¦×”"
    else:
        intro = "×”× ×” ××” ×©××¦××ª×™ ×‘×©×‘×™×œ×š"
    
    # ×”×•×¡×¤×ª ×¤×¨×˜×™× ×¡×¤×¦×™×¤×™×™×
    if topics and not is_more_request:
        intro += f" - ×¤×•×“×§××¡×˜ ×¢×œ {', '.join(topics)}"
    
    if duration and not is_more_request:
        intro += f" ×¢×“ {duration} ×“×§×•×ª"
    
    if language == 'english' and not is_more_request:
        intro += " ×‘×× ×’×œ×™×ª"
    elif language == 'hebrew' and not is_more_request:
        intro += " ×‘×¢×‘×¨×™×ª"
    
    intro += ":"
    return intro

def format_single_recommendation(recommendation, intro_message):
    """×¢×™×¦×•×‘ ×”××œ×¦×” ×‘×•×“×“×ª ×œ×ª×¦×•×’×” ×¢× ×”×§×“××” ××™×©×™×ª ×•×¦×™×•×Ÿ ×“××™×•×Ÿ"""
    if not recommendation:
        return "ğŸ˜” × ×’××¨×• ×”×”××œ×¦×•×ª ×‘× ×•×©× ×–×”. × ×¡×” ×œ×—×¤×© × ×•×©× ××—×¨ ××• ×ª××¨ ××—×¨×ª ××ª ××” ×©××ª×” ××—×¤×©."
    
    rec = recommendation[0]
    
    # ×”×©×ª××© ×‘×”×§×“××” ×”××™×©×™×ª
    text = intro_message + "\n" + "=" * 30 + "\n"
    
    text += f"\nğŸ§ **{rec['name']}**\n"
    text += f"ğŸ‘¤ ×××ª: {rec['publisher']}\n"
     
    # ×©×¤×”
    languages = rec.get('languages', [])
    if 'he' in languages or 'iw' in languages:
        text += f"ğŸ—£ï¸ ×©×¤×”: ×¢×‘×¨×™×ª\n"
    elif 'en' in languages:
        text += f"ğŸ—£ï¸ ×©×¤×”: ×× ×’×œ×™×ª\n"
    
    # ××©×š ×–××Ÿ
    if rec.get('duration_minutes'):
        text += f"â±ï¸ ××©×š: {rec['duration_minutes']} ×“×§×•×ª\n"
    
    # ×ª×™××•×¨
    description = rec['description'][:200] + "..." if len(rec['description']) > 200 else rec['description']
    text += f"ğŸ“ {description}\n"
    text += f"ğŸ”— [×œ×”××–× ×”]({rec['url']})\n"
    
    text += "\n" + "=" * 30 + "\n"
    text += "ğŸ’¡ ×¨×•×¦×” ×¢×•×“ ×”××œ×¦×”? ×¤×©×•×˜ ×ª×›×ª×•×‘ '×¢×•×“' ××• '×¢×•×“ ×”××œ×¦×”'\n"
    text += "ğŸ” ×œ×—×™×¤×•×© × ×•×©× ×—×“×© - ×¤×©×•×˜ ×ª×›×ª×•×‘ ××” ××¢× ×™×™×Ÿ ××•×ª×š\n"
    text += "ğŸ”„ ×œ×”×ª×—×œ×” ××—×“×©: /reset"
    
    return text

# Telegram Bot Functions
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    welcome_message = """ğŸ§ ×”×™×™! ×× ×™ SHMALI - ×”×‘×•×˜ ×©×œ×š ×œ×”××œ×¦×•×ª ×¤×•×“×§××¡×˜×™×!

ğŸ’¬ ×¡×¤×¨ ×œ×™ ×‘××“×•×™×§ ××” ××ª×” ××—×¤×©:

×“×•×’×××•×ª:
â€¢ "×¨×•×¦×” ×œ×©××•×¢ ×¤×•×“×§××¡×˜ ×¢×œ ×¡×¤×•×¨×˜"
â€¢ "×¨×•×¦×” ×œ×©××•×¢ ×¤×•×“×§××¡×˜ ×¢×œ ×‘×•×¨×¡×” ×‘×¢×‘×¨×™×ª ×¢×“ ×—×¦×™ ×©×¢×”"
â€¢ "×¤×•×“×§××¡×˜ ×¢×œ ×˜×›× ×•×œ×•×’×™×” ×‘×× ×’×œ×™×ª"
â€¢ "×¤×•×“×§××¡×˜ ×‘×¢×‘×¨×™×ª ×¢×œ ×¤×¡×™×›×•×œ×•×’×™×” ×¢×“ ×©×¢×”"

××™×š ×× ×™ ×™×›×•×œ ×œ×¢×–×•×¨ ×œ×š?
*×× ×™ ×œ× ×¢×•×‘×“ ×× ×œ× ×ª×¨×©×•× ×œ×™ ××ª ×”×ª×—×•× ×©×‘×• ××ª×” ××—×¤×©*"""
   
    await update.message.reply_text(welcome_message)

async def reset(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """××™×¤×•×¡ ×”×©×™×—×”"""
    user_id = update.effective_user.id
    bot_instance = context.bot_data['bot_instance']
    if user_id in bot_instance.gpt_analyzer.conversation_history:
        del bot_instance.gpt_analyzer.conversation_history[user_id]
    if user_id in bot_instance.shown_recommendations:
        del bot_instance.shown_recommendations[user_id]
    if user_id in bot_instance.available_recommendations:
        del bot_instance.available_recommendations[user_id]

    welcome_message = """ğŸ§ ×”×™×™! ×× ×™ SHMALI - ×”×‘×•×˜ ×©×œ×š ×œ×”××œ×¦×•×ª ×¤×•×“×§××¡×˜×™×!

ğŸ’¬ ×¡×¤×¨ ×œ×™ ×‘××“×•×™×§ ××” ××ª×” ××—×¤×©:

×“×•×’×××•×ª:
â€¢ "×¨×•×¦×” ×œ×©××•×¢ ×¤×•×“×§××¡×˜ ×¢×œ ×¡×¤×•×¨×˜"
â€¢ "×¨×•×¦×” ×œ×©××•×¢ ×¤×•×“×§××¡×˜ ×¢×œ ×‘×•×¨×¡×” ×‘×¢×‘×¨×™×ª ×¢×“ ×—×¦×™ ×©×¢×”"
â€¢ "×¤×•×“×§××¡×˜ ×¢×œ ×˜×›× ×•×œ×•×’×™×” ×‘×× ×’×œ×™×ª"
â€¢ "×¤×•×“×§××¡×˜ ×‘×¢×‘×¨×™×ª ×¢×œ ×¤×¡×™×›×•×œ×•×’×™×” ×¢×“ ×©×¢×”"

××™×š ×× ×™ ×™×›×•×œ ×œ×¢×–×•×¨ ×œ×š?
*×× ×™ ×œ× ×¢×•×‘×“ ×× ×œ× ×ª×¨×©×•× ×œ×™ ××ª ×”×ª×—×•× ×©×‘×• ××ª×” ××—×¤×©*"""

    await update.message.reply_text("ğŸ”„ ×”×©×™×—×” ××•×¤×¡×”! \n\n" + welcome_message)

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """×˜×™×¤×•×œ ×‘×”×•×“×¢×•×ª ×˜×§×¡×˜ ×¢× ×‘×“×™×§×ª ×¨×œ×•×•× ×˜×™×•×ª ×œ×¤×•×“×§××¡×˜×™×"""
    user_text = update.message.text
    user_id = update.effective_user.id
    bot_instance = context.bot_data['bot_instance']
    
    # ×‘×“×™×§×” ×× ×”××©×ª××© ××‘×§×© ×¢×•×“ ×”××œ×¦×•×ª
    is_more_request = any(word in user_text.lower() for word in ['×¢×•×“', '×”××œ×¦×” × ×•×¡×¤×ª', '×¢×•×“ ×”××œ×¦×”', '×”×‘×', 'next'])
    
    # ×× ××‘×§×©×™× ×¢×•×“ ×•×”×™×” × ×™×ª×•×— ×§×•×“×
    if is_more_request and user_id in bot_instance.gpt_analyzer.conversation_history:
        history = bot_instance.gpt_analyzer.conversation_history[user_id]
        if history:
            # ×”×©×ª××© ×‘× ×™×ª×•×— ×”××—×¨×•×Ÿ
            analysis = history[-1]['analysis']
            
            # ×•×•×“× ×©×”× ×™×ª×•×— ×”×§×•×“× ×”×™×” ×¢×œ ×¤×•×“×§××¡×˜×™×
            if not analysis.get('is_podcast_related', True):
                await update.message.reply_text("ğŸ¤” ×œ× ××¦××ª×™ ×”×™×¡×˜×•×¨×™×” ×©×œ ×—×™×¤×•×© ×¤×•×“×§××¡×˜×™×. ×× × ×ª××¨ ××™×–×” ×¤×•×“×§××¡×˜ ××ª×” ××—×¤×©.")
                return
                
            # ×”××©×š ×¢× ×”× ×™×ª×•×— ×”×§×™×™×
            print(f"ğŸ“± User {user_id} requested 'more' - using existing analysis")
        else:
            await update.message.reply_text("ğŸ¤” ×œ× ××¦××ª×™ ×”×™×¡×˜×•×¨×™×” ×©×œ ×—×™×¤×•×©. ×× × ×ª××¨ ××” ××ª×” ××—×¤×©.")
            return
    else:
        # ×¨×§ ×× ×–×• ×œ× ×‘×§×©×” ×œ"×¢×•×“" - ×¢×©×” ×‘×“×™×§×ª ×©×¤×” ×•× ×™×ª×•×— ×—×“×©
        
        # ×‘×“×™×§×” ×©×”×”×•×“×¢×” ×‘×¢×‘×¨×™×ª
        hebrew_chars = sum(1 for char in user_text if '\u0590' <= char <= '\u05FF')
        if hebrew_chars < len(user_text) * 0.2:
            await update.message.reply_text(
                "ğŸ¤– ×× ×™ ××‘×™×Ÿ ×¨×§ ×¢×‘×¨×™×ª. ×× × ×›×ª×•×‘ ×‘×¢×‘×¨×™×ª ××” ××ª×” ××—×¤×©."
            )
            return
        
        # × ×™×ª×•×— ×”×‘×§×©×” ×”×—×“×©×”
        print(f"ğŸ“± User {user_id} sent new request: '{user_text}' - analyzing...")
        analysis = await bot_instance.gpt_analyzer.analyze_request(user_text, user_id)
        
        # ×‘×“×™×§×” ×× ×”×‘×§×©×” ×§×©×•×¨×” ×œ×¤×•×“×§××¡×˜×™×
        if not analysis.get('is_podcast_related', True):
            suggested_response = analysis.get('suggested_response', 
                "ğŸ¤– ×× ×™ ××ª××—×” ×¨×§ ×‘××ª×Ÿ ×”××œ×¦×•×ª ×¢×œ ×¤×•×“×§××¡×˜×™×.\n\n×¡×¤×¨ ×œ×™ ××™×–×” × ×•×©× ××¢× ×™×™×Ÿ ××•×ª×š ×œ×©××•×¢ ×‘×¤×•×“×§××¡×˜! ğŸ§")
            await update.message.reply_text(suggested_response)
            return
    
    await update.message.chat.send_action("typing")
    
    try:
        # ×§×‘×œ×ª ×”××œ×¦×” ××—×ª
        recommendation = await bot_instance.get_recommendations(analysis, user_id)
        
        # ×™×¦×™×¨×ª ×”×§×“××” ××™×©×™×ª ×¢×œ ×‘×¡×™×¡ ×”× ×™×ª×•×—
        intro_message = create_personalized_intro(analysis, is_more_request)
        
        # ×‘× ×™×™×ª ×”×ª×’×•×‘×” ×”×××•×—×“×ª
        response = format_single_recommendation(recommendation, intro_message)
        
        await update.message.reply_text(
            response,
            parse_mode="Markdown",
            disable_web_page_preview=True
        )
        
        print(f"âœ… Sent recommendation to user {user_id}")
        
    except Exception as e:
        print(f"âŒ Error handling message: {e}")
        await update.message.reply_text(
            "ğŸ˜… ××©×”×• ×”×©×ª×‘×©. ×ª× ×¡×” ×©×•×‘?"
        )

def main():
    # ×‘×“×™×§×ª ×”×’×“×¨×•×ª
    if OPENAI_API_KEY == 'YOUR_OPENAI_API_KEY_HERE':
        print("âŒ ×™×© ×œ×”×’×“×™×¨ ××ª OPENAI_API_KEY!")
        return
    
    app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
    bot = ShmaliBot()
    app.bot_data["bot_instance"] = bot
    
    # ×”×•×¡×¤×ª handlers
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("reset", reset))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    
    print("ğŸš€ SHMALI Bot ××ª×—×™×œ!")
    print("ğŸ¤– GPT ××•×¤×¢×œ ×¢× ×‘×“×™×§×ª ×¨×œ×•×•× ×˜×™×•×ª")
    print("ğŸ”¤ ×–×™×”×•×™ ×©×¤×” ××©×•×¤×¨")
    print("ğŸ“Š ××¢×¨×›×ª Similarity Scoring (70%-30%)")
    print("ğŸ¯ ×“×™×¨×•×’ ×—×›× ×©×œ ×ª×•×¦××•×ª")
    print("ğŸ”„ ×”×’× ×” ××¤× ×™ ×‘×§×©×•×ª ×œ× ×¨×œ×•×•× ×˜×™×•×ª")
    print("ğŸ› ××¦×‘ Debug ××•×¤×¢×œ - ×ª×¨××” ×œ×•×’×™× ××¤×•×¨×˜×™×")
    
    app.run_polling()

if __name__ == '__main__':
    main()
